[[objectives]]
== Objectives and Background

Saturn was developed with the following objectives:

 * Provide a *representative baseline* implementation of the RISC-V Vector specification
 * Support *full compliance* with the complete RVV specification, including support for virtual memory and precise traps
 * Target performant *ASIC implementations*, rather than FPGA deployments
 * Be sufficiently *parameterized* to enable design-space-exploration across a wide power/performance/area design space of silicon implementations
 * Demonstrate efficient scheduling of vector operations on a microarchitecture with a *short hardware vector length*
 * Integrate with existing *efficient area-compact scalar core* implementations
 * Support *extensibility* with custom vector instructions, functional units, and accelerators that leverage the baseline capability in the standard RVV ISA
 * Target deployment as part of a *DSP core* or similarly domain-specialized core, instead of general-purpose systems

We present a background discussion of deployment scenarios for data-parallel systems as well as the dominant architectures in the commercial space for such systems.
We highlight how Saturn, as a implementation of a modern scalable vector ISA targeting deployment in specialized cores, fills an underexplored niche in the space of data-parallel microarchitectures.

=== Terminology

In this document, we use the term "DSP" extensively. This term is slightly overloaded, as it may refer in the literature to the domain of "Digital Signal Processing" or to "Digital Signal Processor" cores.
We generally use "DSP" in this document as a catch-all term to refer to the workloads and kernels in the digital signal processing domain, as well as adjacent domains that include image processing and machine learning.
We use this terminology because of the similarity of many of the underlying kernels across these domains and the paradigms that are used to compute them in modern SoCs.
We will be more precise and explicit where it is needed.

=== Data-parallel Workloads


We broadly categorize the range of deployment scenarios and workloads for programmable data-parallel systems into four domains.
These categories are differentiated by power/area constraints and workload characteristics.


[.text-center]
.Taxonomy of data-parallel domains and their dominant architectures
image::diag/domains.png[DLP Domains,align=center,title-align=center]


[discrete]
==== HPC, Scientific Computing, and Datacenter ML

HPC, scientific computing, and ML training workloads are typically characterized by very long application vector lengths, exhibiting a high degree of data-level parallelism.
Systems targeting these workloads are typically designed and deployed at datacenter scale, with available data-level-parallelism exploited across the many nodes in a warehouse-scale computer.


[discrete]
==== General-purpose Server, Desktop, and Mobile

General-purpose server, desktop, and mobile workloads exhibit a modest degree of available data-level-parallelism interspersed between many blocks of sequential, non-data-parallel code.
The implications of Amdahl's Law dictate the design of these systems; exploiting data-level parallelism must not come at the expense of single-thread performance.
Compare to the other domains, compiler and programmer-friendly ISAs are particularly important for these systems, as they must remain performant on the greatest diversity of workloads.

[discrete]
==== Digital Signal Processing and Domain-Specialized

Digital signal processing (DSP) and similar domain-specialized workloads feature a modest-to-high degree of data-level parallelism, and additionally often have strict latency requirements.
These workloads are often offloaded in modern SoCs to specialized "DSP cores" optimized towards power and area-efficient expoitation of data-level parallelism.
Large modern SoCs typically integrate many of such "DSP cores" across a variety of design points to provide optimized compute for different subsystems of the chip.
To meet the application QoS requirements and system-wide power/area requirements, these cores must target extreme power and area efficiency while maintaining high utilization of data-parallel functional units.

[discrete]
==== Embedded

Embedded applications executing on embedded cores face extreme constraints around power and area, as well as code size.
Exploiting data-level parallelism can be critical for these workloads as well, but broadly comes as a secondary concern compared to power and area.
As these cores often provide a self-contained memory system, code and data size are critical architectural considerations.

=== Data-parallel Architectures

The requirements of the common data-parallel workload domains have induced the commercial space to reduce towards several fundamental architectural archetypes.

[discrete]
==== Long-vector Implementations

*Long-vector ISAs* and their *many-lane vector-unit* implementations trace their heritage most clearly to the early Cray-style vector units.
Modern incarnations of such systems primarily target the HPC and datacenter space, where long application vector lengths map cleanly into high-capacity vector-register files.

Microarchitecturally, these implementations distribute the vector register file, functional units, and memory units across many vector lanes, with scalability towards many lanes being a key requirement.
A single core of such a vector unit might have upwards of 32 parallel vector lanes, with a hardware vector length of over 16384 bits.

The NEC Aurora is a modern example of a commercial long-vector microarchitecture, while Hwacha, Ara, and Vitruvius are examples of academic long-vector implementations.


[discrete]
==== SIMT GPGPUs

*GPGPUs* execute a specialized *SIMT* ISA across a multi-core of multi-threaded processors.
GPGPUs evolved from the development of general-purpose SIMT programming models for graphics shader pipelines.
By scaling the number of multi-threaded cores, GPGPU microarchitectures can target integrated GPUs in general-purpose SoCs, graphics accelerator cards for desktop/consumer systems, as well as interconnected compute cards for datacenter-scale HPC systems.
The rise of GPGPUs can be associated with the decline of specialized long-vector microarchitectures for datacenter HPC systems.

NVIDIA and AMD dominate the GPGPU market.
Their HPC and datacenter products, while originally derived from consumer GPU architectures, have since diverged towards a more differentiated, yet still multi-core-SMT-SIMT microarchitecture.

Within the general-purpose mobile consumer space, AMD, Intel, Apple, Qualcomm, and Huawei all deploy embedded GPGPU systems within their SoCs.
These vendors all provide developer SDKs for supporting general purpose applications on their GPGPU microarchitectures.

[discrete]
==== General-purpose ISAs with SIMD

*General-purpose ISAs with SIMD extensions* enable exploiting data-level parallelism in a microarchitecture already optimized towards scalar IPC.
SIMD extensions evolved from subword-SIMD ISAs designed to pack a small vector of sub-word elements within a single scalar register as a cheap way to integrate
limited forms of data-parallel compute into a general-purpose processor.

Microarchitectures of general-purpose cores perform instruction scheduling of such SIMD instructions using similar mechanisms as they do for scalar instructions; namely, existing features like superscalar issue, out-of-order execution, and register renaming are all reused to maintain high instruction throughput into the SIMD functional units.
This paradigm has largely held true even towards modern general-purpose-SIMD cores, which feature much wider SIMD registers than scalar registers for improved performance on data-parallel workloads.

Practically all relevant commercial general-purpose cores ship with SIMD extensions.
All Intel and AMD out-of-order cores support some form of the SSE or AVX extensions, while ARM's A-profile architecture requires NEON.

[discrete]
==== VLIW ISAs with SIMD

*VLIW ISAs with SIMD extensions have traditionally dominated the space of DSP and similar domain-specialized cores, as well as embedded cores.
For these deployment scenarios, power and area constraints preclude the deployment of costly microarchitectural features towards high instruction throughput in a general-purpose ISA, such as speculative execution, register renaming, and hardware interlocks.

Additionally, the regularity present in many DSP kernels lend themselves to the VLIW paradigm, and applications for domain-specialied cores benefit less from the compiler ergonomics afforded by general-purpose ISAs.
Despite this, VLIW-based DSPs are notoriously difficult to program and can suffer from issues such as large static code size arising from the need for extensive static scheduling.
Nonetheless, specialized VLIW ISAs with efficient hardware implementations provide a cheap and programmer-visible mechanism for maintaining high instruction throughput into SIMD functional units.

Cadence, CEVA, and Qualcomm all ship commercial VLIW DSPs with SIMD extensions.
Cadence and CEVA cores are IP products typically integrated into a customer's SoC as an embedded core, while Qualcomm's Hexagon DSP Cores are integrated throughout their SoC line to provide DSP compute.

[discrete]
==== Scalable Vector ISAs

In contrast to the above patterns, modern *scalable vector ISAs* aspire to provide a common unified ISA that can support a range of microarchitectural implementation styles, supporting long-vector Cray-like machines, general-purpose out-of-order machines with vector extensions, specialized DSP cores with vector extensions, as well as ultra-compact embedded vector units.
The dominant examples of such ISAs include ARM's proprietary SVE and MVE extensions, as well as the open RISC-V Vector extension.

Existing academic implementations of RVV have broadly targeted the HPC and general-purpose deployment scenarios.
Compared to prior academic implementations, Saturn targets DSP and domain-specialized cores, and represents a class of designs we call *"short-vector"*.
Saturn demonstrates that these "short-vector" designs, without the distributed-lane microarchitecture of the long-vector units, can still retain high performance and efficiency for vector kernels.
Notably, Saturn also demonstrates that these "short-vector" designs do not need to sacrifice any fundamental requirements of modern vector ISAs to be performant and efficient.


=== The RISC-V Vector ISA

The RISC-V Vector ISA is the standard extension in RISC-V for exploiting data-level parallelism.
A full discussion of the ISA design can be found in its specification.
In this section, we highlight several properties of RVV that pose notable challenges to implementation.

[discrete]
==== Dynamic `VTYPE`/`VL`

Stripmine loops in RVV use `vset` instructions to dynamically adjust vector configuration state in the body of the loops.
These instructions set the dynamic `vl` vector length register in addition to the `vtype` register, which sets the element width, register grouping, and mask/tail agnosticity for subsequent operations.

While a naive implementation of RVV might treat the `vtype` as a single system-wide register, owing to its effect on the behavior of many components of the vector datapath, such an approach would substantially degrade performance.
As `vset` is used in inner-loops, performant implementations must dynamically track the `vtype` and `vl` registers, instead of maintaining a single global copy.
Since neither `vtype` nor `vl` require many bits to encode, this state can be renamed into a control bundle that propagates alongside each vector instruction in the datapath.

Furthermore, since `vtype` and `vl` affect the generation of precise traps by vector memory instructions, it is insufficent to update these registers only at commit, since precise-traps must be generated ahead-of-commit.
Doing so would introduce an interlock between a `vset` and a subsequent vector memory operation, which must stall until the `vset` commits before using the updated `vtype`/`vl` to check for precise traps.
Instead, performant scalar core implementations should bypass updates of `vtype` and `vset` to an early stage in the pipeline to avoid this interlock.

[discrete]
==== Memory Ordering

RVV mandates that vector memory operations appear to execute in instruction order with respect to *all* other instructions on the same hart, including scalar memory instructions.
While an alternative approach may have relaxed this ordering requirement, such an approach would necessitate costly and precise programmer-inserted fences to enforce scalar-vector memory ordering.

This ordering requirement poses a challenge to decoupled post-commit vector unit implementations, in which vector loads and stores might run behind scalar loads and stores.
Stalling scalar loads and stores until the vector loads and stores drain could have costly implications on kernels which naturally would benefit from overlapping scalar and vector memory operations.
For instance, an input-stationary matrix-multiplication, where the inner loop streams across scalar load of one the inputs, and vector loads and store of the accumulator, naturally leads to scalar-vector memory overlap.

Performant implementations should allow concurrent execution of scalar and vector memory operations when it can be precisely determined that the accessed regions do not overlap, and thus cannot  violate the memory ordering requirements.

[discrete]
==== Precise Traps

RVV mandates precise traps for vector memory operations.
Vector loads and stores which generate a trap must execute up until the element which causes the trap, report the precise element index which generated the trap, and generate that trap precisely in the instruction stream.
This implies that implementations must check for precise traps ahead-of-commit.

However, offloading address-generation entirely ahead-of-commit would have significant negative performance consequences, as this would stall unrelated scalar instructions even in the common-case where instructions do not trap.
Performant implementations should expediantly commit vector memory instructions in the common case where they do not trap, and only interlock the scalar core in the uncommon case where a trap is present.

[discrete]
==== `LMUL` Register Grouping

The `LMUL` (length multiplier) register grouping field of `VTYPE` enables grouping of consecutive vector registers into a single longer vector register.
In addition to enabling mixed-precision operations, this feature also allows kernels which don't induce vector register pressure to access an effectively longer hardware vector length.
Generally, performance programmers for RISC-V will use this feature to reduce dynamic instruction count of their loops and potentially improve utilization of hardware compute resources.
For example, vector `memcpy` induces no register pressure and can trivially set a high `LMUL` to reduce dynamic instruction count.

Thus, implementations should not penalize code which uses high LMUL, as long as the underlying application vector lengths are long enough to support it.
One approach to implementing this functionality would be to treat LMUL > 1 operations as individual operations by cracking them early in the pipeline.
While this strategy is easy and low-cost to implement, it may cause performance issues, as the many micro-ops from cracked high-`LMUL` instructions would induce greater pressure on datapath scheduling resources.

[discrete]
==== Segmented Memory Operations

Segmented memory operations enable a "transpose" of an "array-of-structs" data representation in memory into a "struct-of-arrays" in consecutive vector registers.
Such operations, while very complex behaviorally, are fundamental to many algorithms and datatypes.
For instance, complex numbers and image pixel data are conventionally stored in memory as "arrays-of-structs".
Such instructions can also be used to perform on-the-fly transposes into vector registers.

These operations can significantly reduce programmer burden, and thus performant RVV implementations should not impose an excess performance overhead from their execution.
To match the performance programming model, vector code which uses these memory operations to reduce dynamic instruction count should perform no worse than the equivalent code which explicitly transforms the data over many vector instructions.

=== Comparing against Saturn

We compare Saturn's architecture and short-vector microarchitecture to the data-parallel paradigms discussed above.

[discrete]
==== Compared to Long-Vector Units

Long-vector microarchitectures for datacenter ML and HPC feature very-long-vector-lengths distributed across many parallel vector lanes.
Such implementations typically store these long vectors in dense SRAM.
Given the very long vector lengths, a single vector instruction might encode many cycles of work, even across parallel vector lanes.
Thus, instruction throughput is less critical for maintaining high utilization of functional units.
Instead, long-vector microarchitectures typically derive efficiency and high utilization by amortizing costs over fewer inflight instructions that are executed in a deep temporal fashion.

In constrast, Saturn's short-vector microarchitecture implements a unified wide vector register file with a unified SIMD datapath and load-store path.
Saturn does not utilize deep temporal execution of vector instructions to achieve efficiency.
For example, code sequences with low `LMUL` might require only 1-2 cycles of occupancy in the functional units per instruction.
Instead, it relies on precise sequencing mechanisms to achieve high datapath utilization with shorter vectors.
Note that this does imply that Saturn supports higher instruction throughput than would be necessary in a long-vector microarchitecture.

We argue that this short-vectors paradigm is better suited than long-vectors implementations to build DSP-style cores for several important reasons:

* Many DSP applications feature short and/or widely varying application vector lengths. This makes it difficult for long-vector to effectively utilize their resources as it precludes deep temporal execution.
Short-vector machines can achieve higher utilization on these shorter application vector lengths.
* Short-vector machines use an inherently lower capacity vector register file, which has positive implications in terms of silcon area and power consumption.
* Saturn efficiently implements the register grouping functionality of RVV, and thus can still achieve high performance on long application vector lengths when the kernel calls for it.


[discrete]
==== Compared to GPGPUs

GPGPUs exploit data-level-parallelism across many SIMT threads, executing on a multi-core of multi-threaded processors.
In contrast, Saturn's vector ISA exploits data-level-parallelism across many elements in a single vector register, executing on a vector datapath with SIMD functional units.

[discrete]
==== Compared to General-purpose SIMD Cores

SIMD datapaths in general-purpose out-of-order cores are typically deeply integrated into the scalar instruction execution pipeline.
Existing capabilities for out-of-order execution, speculative execution, superscalar fetch, and register renaming are leveraged to maximize SIMD datapath utilization.
While these features are costly in power and area, they are fundamental necessary components of modern general-purpose cores, and thus are also leveraged when executing SIMD code.

Unlike these cores, Saturn's short-vector design does not rely on high-performance features of the scalar core and instruction fetch.
By leveraging efficient scheduling of short-chime vector instructions, with limited capability for out-of-order execution, Saturn remains performant even with a minimal in-order scalar core.
Efficient and precise vector operation scheduling, rather than high instruction throughput, is key to maintaining SIMD datapath utilization.

[discrete]
==== Compared to VLIW + SIMD DSP Cores

VLIW cores with SIMD extensions also rely on high-throughput instruction fetch for performance through VLIW instruction encodings.
VLIW instruction encodings enable low-cost, low-complexity superscalar fetch and provide the programmer precise control over instruction scheduling.

Unlike instructions in SIMD ISAs, instructions in Saturn's vector ISA are executed over multiple cycles in multi-cycle "chime".
Since a single instruction can occupy a functional unit or similar structural resource over multiple cycles, vector instruction throughput can be less than one per cycle while maintaining full utilization.
Thus, Saturn can remain performant with a narrow in-order host scalar core executing a general-purpose ISA.

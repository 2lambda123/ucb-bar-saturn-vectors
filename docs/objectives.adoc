[[objectives]]
== Objectives and Background

Saturn was developed with the following objectives:

 * Provide a *representative baseline* implementation of the RISC-V Vector specification
 * Support *full compliance* with the complete RVV specification, including support for virtual memory and precise traps
 * Be sufficiently *parameterized* to enable design-space-exploration across a wide range of the power/performance/area design space
 * Demonstrate efficient scheduling of vector operations on a microarchitecture with a *short hardware vector length*
 * Integrate with existing *efficient area-compact scalar core* implementations
 * Support *extensibility* with custom vector instructions, functional units, and accelerators that leverage the baseline capability in the standard RVV ISA
 * Target deployment as part of a *DSP core* or similarly domain-specialized core, instead of general-purpose systems

We present a background discussion of deployment scenarios for data-parallel systems as well as the dominant architectures in the commercial space for such systems.
We highlight how Saturn, as a implementation of a modern scalable vector ISA targeting deployment in specialized cores, fills an underexplored niche in the space of data-parallel microarchitectures.

=== Data-parallel Workloads

We broadly categorize the range of deployment scenarios and workloads for programmable data-parallel systems into four categories.
These categories are differentiated by power/area constraints and workload characteristics.

[discrete]
==== HPC, Scientific Computing, and Datacenter ML

HPC, scientific computing, and ML training workloads are typically characterized by very long application vector lengths, exhibiting a high degree of data-level parallelism.
Systems targeting these workloads are typically designed and deployed at datacenter scale, with available data-level-parallelism exploited across the many nodes in a system.

[discrete]
==== General-purpose Server, Desktop, and Mobile

General-purpose server, desktop, and mobile workloads exhibit a modest degree of available data-level-parallelism interspersed between many blocks of scalar, non-data-parallel code.
In these systems, single-thread performance remains critical due to Amdahl's law, so data-parallel architectures and microarchitectures must not come at the expense of scalar performance in general-purpose code.

[discrete]
==== Digital Signal Processing and Domain-Specialized

Digital signal processing (DSP) and similar domain-specialized workloads feature a modest-to-high degree of data-level parallelism, and additionally often have strict latency requirements.
These workloads are often offloaded in modern SoCs to specialized "DSP cores" optimized towards power and area-efficient expoitation of data-level parallelism.
To meet the application QoS requirements and system-wide power/area requirements, these cores must target extreme power and area efficiency while maintaining high utilization of data-parallel functional units.

[discrete]
==== Embedded

Embedded applications executing on embedded cores face extreme constraints around power and area.
Exploiting data-level parallelism can be critical for these workloads as well, but broadly comes as a secondary concern compared to power and area.

=== Data-parallel Architectures

We taxonomize the range of commonly deployed architectural and microarchitectural paradigms for each of the above deployment scenarios.

[discrete]
==== Long-vector Implementations

*Long-vector ISAs* and their *many-lane vector-unit* implementations trace their heritage most clearly to the early Cray-style vector units.
Modern incarnations of such systems primarily target the HPC and datacenter space, where long application vector lengths map cleanly into high-capacity vector-register files.
Microarchitecturally, these implementations distribute the vector register file, functional units, and memory units across many vector lanes, with scalability towards many lanes being a key requirement.
A single core of such a vector unit might have upwards of 32 parallel vector lanes, with a hardware vector length of over 16384 bits.
The NEC Aurora is a modern example of a commercial long-vector microarchitecture, while Hwacha, Ara, and Vitruvius are examples of academic long-vector implementations.

[discrete]
==== SIMT GPGPUs

*GPGPUs* execute a specialized *SIMT* ISA across a multi-core of multi-threaded processors.
GPGPUs evolved from the development of general-purpose SIMT programming models for graphics pipelines.
By scaling the number of multi-threaded cores, GPGPU microarchitectures can target integrated GPUs in general-purpose SoCs, graphics accelerator cards for desktop/consumer systems, as well as interconnected compute cards for datacenter-scale HPC systems.
The rise of GPGPUs has partially led to the decline of specialized long-vector microarchitectures for datacenter HPC systems.

[discrete]
==== General-purpose ISAs with SIMD

*General-purpose ISAs with SIMD extensions* enable exploiting data-level parallelism in a microarchitecture already optimized towards scalar IPC.
SIMD extensions evolved from subword-SIMD ISAs designed to pack a small vector of sub-word elements within a single scalar register.
Microarchitectures of general-purpose cores perform instruction scheduling of such SIMD instructions using similar mechanisms as they do for scalar instructions; namely, existing features like superscalar issue, out-of-order execution, and register renaming are all reused to maintain high instruction throughput into the SIMD functional units.
This paradigm has largely held true even towards modern general-purpose-SIMD cores, which feature much wider SIMD registers than scalar registers for improved performance on data-parallel codes.

[discrete]
==== VLIW ISAs with SIMD

*VLIW ISAs with SIMD extensions* dominate the space of DSP and similar domain-specialized cores, as well as embedded cores.
For these deployment scenarios, power and area constraints preclude the deployment of costly microarchitectural features towards instruction throughput in a general-purpose ISA, such as speculative execution, register renaming, and hardware interlocks.
Conversely, applications for domain-specialied cores benefit less from the compiler ergonomics afforded by general-purpose ISAs.
Thus, specialized VLIW ISAs with efficient hardware implementations provide a cheap and programmer-visible mechanism for maitaining high instruction throughput into SIMD functional units.

[discrete]
==== Scalable Vector ISAs

In contrast to the above patterns, modern *scalable vector ISAs* aspire to provide a common unified ISA that can support a range of microarchitectural implementation styles, supporting long-vector Cray-like machines, general-purpose out-of-order machines with vector extensions, specialized DSP cores with vector extensions, as well as ultra-compact embedded vector units.
The dominant examples of such ISAs include ARM's proprietary SVE and MVE extensions, as well as the open RISC-V Vector extension.

Existing academic implementations of RVV have broadly targeted the HPC and general-purpose deployment scenarios.
Compared to prior academic implementations, Saturn targets DSP and domain-specialized cores, and represents a class of designs we call *"short-vector"*.


=== The RISC-V Vector ISA

The RISC-V Vector ISA is the standard extension in RISC-V for exploiting data-level parallelism.
A full discussion of the ISA design can be found in its specification.
In this section, we highlight several properties of RVV that pose notable challenges to implementation.

[discrete]
==== Dynamic `VTYPE`/`VL`

Stripmine loops in RVV use `vset` instructions to dynamically adjust vector configuration state in the body of the loops.
These instructions set the dynamic `vl` vector length register in addition to the `vtype` register, which sets the element width, register grouping, and mask/tail agnosticity for subsequent operations.

While a naive implementation of RVV might treat the `vtype` as a single system-wide register, owing to its effect on the behavior of many components of the vector datapath, such an approach would substantially degrade performance.
As `vset` is used in inner-loops, performant implementations must effectively rename the `vtype` and `vl` registers, instead of maintaining a single global copy.
Since neither `vtype` nor `vl` require many bits to encode, this state can be renamed into a control bundle that propagates alongside each vector instruction in the datapath.

Furthermore, since `vtype` and `vl` affect the generation of precise traps by vector memory instructions, it is insufficent to update these registers only at commit, since precise-traps must be generated ahead-of-commit.
Doing so would introduce an interlock between a `vset` and a subsequent vector memory operation, which must stall until the `vset` commits before using the updated `vtype`/`vl` to check for precise traps.
Instead, performant scalar core implementations should bypass updates of `vtype` and `vset` to an early stage in the pipeline to avoid this interlock.

[discrete]
==== Memory Ordering

RVV mandates that vector memory operations appear to execute in instruction order with respect to *all* other instructions on the same hart, including scalar memory instructions.
While an alternative approach may have relaxed this ordering requirement, such an approach would necessitate costly and precise programmer-inserted fences to enforce scalar-vector memory ordering.

For implementations, the ordering requirement poses a challenge to decoupled post-commit vector units, in which vector laods and store might run behind scalar loads and stores.
Stalling scalar loads and stores until the vector loads and stores drain could have costly implications on kernels which naturally would benefit from overlapping scalar and vector memory operations.
For instance, a input-stationary matrix-multiplication, where the inner loop streams across scalar load of one the inputs, and vector loads and store of the accumulator, naturally leads to scalar-vector memory overlap.

Performant implementations should allow concurrent execution of scalar and vector memory operations when it can be precisely determined that the accessed regions do not overlap, and thus do not violate the memory ordering requirements.

[discrete]
==== Precise Traps

RVV mandates precise traps for vector memory operations.
Vector loads and stores which generate a trap must execute up until the element which causes the trap, report the precise element index which generated the trap, and generate that trap precisely in the instruction stream.
This implies that implementations must check for precise traps ahead-of-commit.

However, offloading address-generation entirely ahead-of-commit would have significant negative performance consequences, as this would stall unrelated scalar instructions even in the common-case where instructions do not trap.
Performant implementations should expediantly commit vector memory instructions in the common case where they do not trap, and only interlock the scalar core in the uncommon case where a trap is present.

[discrete]
==== `LMUL` Register Grouping

The `LMUL` (length multiplier) register grouping field of `VTYPE` enables grouping of consecutive vector registers into a single longer vector register.
In addition to enabling mixed-precision operations, this feature also allows kernels which don't induce vector register pressure to access an effectively longer hardware vector length.
Generally, performance programmers for RISC-V will use this feature to reduce dynamic instruction count of their loops.
It is also true that many common vector kernels will want to use register grouping.
For example, vector `memcpy` induces no register pressure and can trivially set a high `LMUL` to reduce dynamic instruction count.

Thus, implementations should not penalize code which uses high LMUL.
A naive approach of instruction cracking early in the pipeline, while easy and low-cost to implement, would violate this requirement, as the many micro-ops from cracked high-`LMUL` instructions would induce greater pressure on datapath scheduling resources.

[discrete]
==== Segmented Memory Operations

Segmented memory operations enable a "transpose" of an "array-of-structs" data representation in memory into a "struct-of-arrays" in consecutive vector registers.
Such operations, while very complex behaviorally, are fundamental to many algorithms and datatypes.
For instance, complex numbers and image pixel data are conventionally stored in memory as "arrays-of-structs".
Such instructions can also be used to perform on-the-fly transposes into vector registers.

Given the importance of these instructions, performant RVV implementations should not impose an excess performance overhead from their execution.
To match the performance programming model, vector code which uses these memory operations to reduce dynamic instruction count should perform no worse than the equivalent code which explicitly transforms the data over many vector instructions.

=== Comparing Saturn

We compare Saturn's architecture and short-vector microarchitecture to the existing data-parallel paradigms discussed above.

[discrete]
==== Compared to Long-Vector Units

Long-vector microarchitectures for datacenter ML and HPC feature very-long-vector-lengths distributed across many parallel vector lanes.
Such implementations typically store these long vectors in dense SRAM.
Given the very long vector lengths, a single vector instruction might encode many cycles of work, even across parallel vector lanes.
Thus, instruction throughput is less critical for maintaining high utilization of functional units.
Instead, long-vector microarchitectures can remain performant with few-inflight instructions, as long as a precise and efficient execution schedule is determined for those instructions.

In constrast, Saturn's short-vector microarchitecture implements a unified wide vector register file with a unified SIMD datapath and load-store path.
Saturn also does not have the luxury of deep temporal execution of all vector instructions.
Code sequences with low `LMUL` might require only 1-2 cycles of occupancy in the functional units per instruction.
To remain performant for these sequences, Saturn supports higher instruction throughput than would be necessary in a long-vector microarchitecture.


[discrete]
==== Compared to GPGPUs

GPGPUs exploit data-level-parallelism across many SIMT threads, executing on a multi-core of multi-threaded processors.
In contrast, Saturn's vector ISA exploits data-level-parallelism across many elements in a single vector register, executing on a vector datapath with SIMD functional units.

[discrete]
==== Compared to General-purpose SIMD Cores

SIMD datapaths in general-purpose out-of-order cores are typically deeply integrated into the scalar instruction execution pipeline.
Existing capabilities for out-of-order execution, speculative execution, superscalar fetch, and register renaming are leveraged to maximize SIMD datapath utilization.
While these features are costly in power and area, they are fundamental necessary components of modern general-purpose cores, and thus are also leveraged when executing SIMD code.

Unlike these cores, Saturn's short-vector design does not rely on high-performance features of the scalar core and instruction fetch.
By leveraging efficient scheduling of short-chime vector instructions, with limited capability for out-of-order execution, Saturn remains performant even with a minimal in-order scalar core.
Efficient and precise vector operation scheduling, rather than high instruction throughput, is key to maintaining SIMD datapath utilization.

[discrete]
==== Compared to VLIW + SIMD DSP Cores

VLIW cores with SIMD extensions also rely on high-throughput instruction fetch for performance through VLIW instruction encodings.
VLIW instruction encodings enable low-cost low-complexity superscalar fetch and provide the programmer precise control over instruction scheduling.

Unlike instructions in SIMD ISAs, instructions in Saturn's vector ISA are executed over multiple cycles in multi-cycle "chime".
Since a single instruction can occupy a functional unit or similar structural resource over multiple cycles, vector instruction throughput can be less than one per cycle while maintaining full utilization.
Thus, Saturn can remain performant with a narrow in-order host scalar core executing a general-purpose ISA.
